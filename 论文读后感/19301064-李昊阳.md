## Thoughts on reading the paper

*Google's troika is a widely discussed issue in the industry, and although the old troika has been replaced by the new troika, the significance of studying the old troika is still very important.*

### GFS

The first is GFS, which is a scalable distributed file system for large-scale data-intensive applications. Because it can run on an ordinary PC, its usability and entry barriers are greatly reduced. In terms of design thinking, GFS classified component failure as a common problem at the beginning of the design, and continuous monitoring, error detection, disaster redundancy, and automatic recovery mechanisms are also integrated into GFS as a necessary option. Another point is that the I/O system, block and append data at the end of the file are one of the characteristics of the system, which takes the append operation as the main consideration for performance optimization and atomicity guarantee. Finally, the introduction of atomic record appending operations does not require additional synchronization operations to ensure data consistency. The co-design of application and file system API improves the flexibility of the entire system. The GFS cluster is mainly composed of three parts, which are a single GFS Master node, multiple GFS Chunk (block) servers, and multiple GFS clients. The single node is the predecessor of the nameNode node of the Hadoop underlying storage HDFS. This node mainly manages all file system metadata (name space, access control information, file and Chunk mapping information, current Chunk location information), management system-wide activities (Chunk lease management, orphaned Chunk recycling, Chunk Migration on the Chunk server), sending heartbeat information to maintain communication with each Chunk server (sending instructions, receiving server status information). Multiple GFS Chunk servers are the predecessor of the dataNode node of Hadoop's underlying storage HDFS. It is a Linux machine that allows user-level service processes, including a constant and unique Chunk identifier assigned by the Master server. Since the files stored in GFS are divided into fixed-size Chunks, they are stored in the hard disk in the form of Linux files, and data is read and written according to the Chunk identifier and byte range specified by the GFS client. In terms of reliability, since each block will be replicated to multiple block servers (users can set different replication levels), with redundant backup, the reliability and disaster tolerance of the system are guaranteed. As for the client, I won’t go into details here, but it’s worth mentioning that when multiple GFS clients send requests, the order of the requests is as follows: Send the file name and block index to the Master => Get the specified Chunk ID and location = > The client caches this information with the file name and Chunk index as the key => Send the Chunk ID and byte range to the nearest Chunk server => Get the block data stream. In terms of system interaction, an important principle runs through the entire design, that is, to minimize the interaction between all operations and the Master node. The garbage collection mechanism is also worth learning. GFS will not reclaim the available physical space immediately after the file is deleted. GFS space reclamation uses a lazy strategy, which is only performed during regular garbage collection at the file and Chunk level. Compared with direct deletion, this deletion method provides a consistent and reliable method of clearing useless copies; garbage collection merges the recovery operation of storage space into the regular background activities of the Master node, which is completed in free time, and the response efficiency is high. ; Delayed storage space recovery provides security for accidental and irreversible delete operations.

## Map Reduce

The second is Map Reduce. MapReduce is a software framework based on which applications can be easily written. These applications can run on a large cluster composed of thousands of commercial machines, and are reliable and fault-tolerant. Parallel processing of massive data sets on the terabyte level. What I have to mention is the idea of Mapreduce, which is-"divide and conquer". Among them, Mapper is responsible for the division. One is that the scale of data or calculation should be greatly reduced compared to the original task; the second is the principle of nearby computing, that is, the task will be assigned to the node that stores the required data for calculation; the third is that these small tasks can be calculated in parallel , There is almost no dependency between each other. Then the Reducer is responsible for summarizing the results of the map phase. As for how many Reducers are needed, users can set the value of the parameter mapred.reduce.tasks in the mapred-site.xml configuration file according to the specific problem. The default value is 1. In its entire work process, there are the following four separate entities, which are: 1. Client for submitting MapReduce jobs 2. JobTracker for coordinating job operation 3. TaskTracker 4 for processing tasks after job division . HDFS for sharing work content among other entities. A MapReduce job usually divides the input data set into several independent data blocks, and the Map task processes them in a completely parallel manner. The framework sorts the output of the Map first, and then inputs the result to the Reduce task. Usually the input and output of the job are stored in the file system, and the entire framework is responsible for task scheduling and monitoring, as well as re-executing tasks that have been closed. Generally, the MapReduce framework and the distributed file system run on the same set of nodes, that is, the computing nodes and storage nodes are usually together. This configuration allows the framework to efficiently schedule tasks on those nodes that have stored data, which can make the entire cluster's network bandwidth very efficient. The MapReduce framework operates on <key, value> key-value pairs, that is, the framework regards the input of the job as a set of <key, value> key-value pairs, and also generates a set of <key, value> key-value pairs As the output of the job, the two sets of key-value pairs may be different. On the whole, the workflow of MapReduce can be roughly divided into five steps, which are sharding, formatting, executing MapTask, executing Shuffle, executing ReduceTask, and writing files. In the sharding formatting step, sharding refers to dividing the source file into small data blocks of equal size (128MB by default in Hadoop 2.x). After that, Hadoop will build a Map task for each shard, and use The task runs a custom map() function to process each record in the shard. The data formatting work is to format the divided fragments (split) into data in the form of key-value pairs <key, value>, where key represents the offset, and value represents the content of each line. In the second step, each Map task has a memory buffer (buffer size 100MB), and the intermediate results of the input split data are written into the memory buffer after being processed by the Map task. If the data written by the human reaches the threshold of the memory buffer (80MB), a thread will be started to write the overflow data in the memory to the disk, and the intermediate results of the Map will continue to be written into the buffer at the same time. During the overflow process, the MapReduce framework sorts the keys. If the intermediate result is relatively large, multiple overflow files will be formed, and the final buffer data will all overflow and write to the disk to form an overflow file. If there are multiple Overwrite files, then finally merge all overwritten files into one file. The third step is how the data processed in the Map phase is passed to the Reduce phase in the MapReduce work process. This is a key process in the MapReduce framework. This process is called Shuffle. Shuffle will distribute the processing result data output by MapTask to ReduceTask, and during the distribution process, the data will be partitioned and sorted by key.

## Big Table

The third part is BigTable, which is a sparse, distributed, and persistent storage multi-dimensional sorting Map. The index of the Map is the row key, column key, and timestamp; each value in the Map is an unparsed byte array. (row:string, column:string,time:int64)->string. The row key in the table can be any string. The read or write operations of the same row of keywords are all atomic. Bigtable organizes data through the lexicographical order of row keys. Each row in the table can be dynamically partitioned. Each partition is called a "Tablet", which is the smallest unit for data distribution and load balancing adjustment. In Webtable, by reversing the host name in the URL, web pages under the same domain name can be grouped together and organized into continuous rows. Specifically, we can store the data of maps.google.com/index.html under the keyword com.google.maps/index.html. Storing web pages in the same domain in contiguous areas can make analysis based on the host and domain name more effective. The set of column keys is called the "column family", and the column family is the basic unit of access control. All data stored in the same column family usually belong to the same type. The column family must be created before use, and then data can be stored under any column key in the column family; after the column family is created, data can be stored under any of the column keys. There cannot be too many column families in a table (up to a few hundred), and the column families rarely change during operation; a table can have an unlimited number of columns. In Bigtable, each data item in the table can contain different versions of the same piece of data; different versions of data are indexed by timestamps. The type of the timestamp is a 64-bit integer. You can assign a value to the timestamp to indicate the "real-time" time accurate to milliseconds. In the data items, the data of different versions are sorted in reverse order of the timestamp, that is, the latest data is ranked first. The performance of random read is an order of magnitude or more slower than other operations; the speed of random read operations in memory is much faster (memory reads, do not read the 64KB Block in GFS); the performance of random and sequential write operations is better than random reads ; Directly append the content of the write operation to the end of a Commit log file, and use batch submission to improve performance by writing data to GFS in a stream; random write and sequential write are not too big in performance The difference; the performance of sequence reading is better than random reading. We reduce the number of hard disk accesses by allowing client programs to specify Bloom filters for the SSTable of a specific locality group. We can use the Bloom filter to query whether an SSTable contains specific rows and columns of data. For some specific applications, we only paid a small amount of memory used to store Bloom filters, in exchange for a significantly reduced number of disk accesses for read operations. The use of Bloom filters also implicitly achieves the purpose that most of the time we do not need to access the hard disk when the application accesses a row or column that does not exist. In order to improve the performance of read operations, the Tablet server uses a secondary cache strategy. Scan cache is the first-level cache, which mainly caches the Key-Value pairs obtained by the Tablet server through the SSTable interface; Block cache is the second-level cache, and caches 22 of the SSTable read from GFS. Compared with compressing the entire SSTable, The block compression rate is low Block. For applications that often need to read the same data repeatedly, scanning the cache is very effective; for applications that often need to read the data near the data that has just been read, the block cache is more useful (for example, sequential read, or in Random read different columns in the locality group of a hot row).